{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI4VN hackathon baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTa3Ee15WsJ"
      },
      "source": [
        "\n",
        "\n",
        "# Tổng quan\n",
        "\n",
        "Hướng dẫn này dựa trên hướng dẫn [Transfer learning and fine-tuning](https://www.tensorflow.org/tutorials/images/transfer_learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vecq8AWZVfmH"
      },
      "source": [
        "! git clone https://github.com/hcmcaic/ai4vn-hackathon-2020.git ai4vn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycde9jVtWa42"
      },
      "source": [
        "% cd ai4vn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqOt6Sv7AsMi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from dataloader import image_dataset_from_directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZot8UP3GpHt"
      },
      "source": [
        "!unzip ai4vn_2020.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v77rlkCKW0IJ"
      },
      "source": [
        "## Tiền xử lý dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GoKGm1duzgk"
      },
      "source": [
        "### Tải dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8bSXJfaYDub"
      },
      "source": [
        "# move test_data to test folder to use image_dataset_from_directory function\n",
        "! mkdir ai4vn_2020/test && mv ai4vn_2020/test_data ai4vn_2020/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro4oYaEmxe4r"
      },
      "source": [
        "# ! unzip sample_data.zip\n",
        "\n",
        "path_to_data = 'ai4vn_2020/sample_data'\n",
        "\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "IMG_SIZE = (160, 160)\n",
        "\n",
        "NUM_CLASS = 7\n",
        "\n",
        "train_dataset, train_dataset_filenames = image_dataset_from_directory(path_to_data,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset=\"training\",\n",
        "                                             shuffle=True,\n",
        "                                             seed=505,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAvtLwi7_J__"
      },
      "source": [
        "validation_dataset, validation_dataset_filenames = image_dataset_from_directory(path_to_data,\n",
        "                                                  validation_split=0.2,\n",
        "                                                  subset=\"training\",\n",
        "                                                  shuffle=True,\n",
        "                                                  seed=505,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  image_size=IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3MxDN4WYjZu"
      },
      "source": [
        "test_dataset, test_dataset_filenames = image_dataset_from_directory('ai4vn_2020/test',\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  image_size=IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjSkIb-QGJGa"
      },
      "source": [
        "validation_dataset.class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO1Q2JaW5sIy"
      },
      "source": [
        "Hiển thị 9 hình ảnh và nhãn đầu tiên từ bộ huấn luyện:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5BeQyKThC_Y"
      },
      "source": [
        "class_names = train_dataset.class_names\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MakSrdd--RKg"
      },
      "source": [
        "### Định cấu hình cho tập dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22XWC7yjkZu4"
      },
      "source": [
        "Use buffered prefetching to load images from disk without having I/O become blocking. To learn more about this method see the [data performance](https://www.tensorflow.org/guide/data_performance) guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3UUPdm86LNC"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYfcVwYLiR98"
      },
      "source": [
        "\n",
        "\n",
        "### Sử dụng tăng cường dữ liệu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDWc5Oad1daX"
      },
      "source": [
        "Khi dữ liệu ảnh không đủ lớn, bạn có thể tăng tính đa dạng của các mẫu dữ liệu bằng cách áp các phép biến đổi ngẫu nhiên. Điều này giúp giảm [overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit). Bạn có thể tìm hiểu thêm về tăng dữ liệu trong [hướng dẫn](https://www.tensorflow.org/tutorials/images/data_augmentation) này."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emc1QYPi37ca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P99QiMGit1A"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mD3rE2Lm7-d"
      },
      "source": [
        "Dưới đây là kết quả của việc tăng cường dữ liệu trên một tấm ảnh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQullOUHkm67"
      },
      "source": [
        "for image, _ in train_dataset.take(1):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  first_image = image[0]\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "    plt.imshow(augmented_image[0] / 255)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAywKtuVn8uK"
      },
      "source": [
        "### Rescale pixel values\n",
        "Trong ví dụ này, ta sử dụng mô hình cơ sở tf.keras.applications.MobileNetV2. Mô hình này sử dụng giá trị điểm ảnh trong đoạn `[-1,1]` nhưng giá trị ảnh hiện tại đang trong đoạn `[0-255]`. Vì thế ta cần chuẩn hóa lại miền giá trị điểm ảnh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO0HM9JAQUFq"
      },
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2NyJn4KQMux"
      },
      "source": [
        "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkH-kazQecHB"
      },
      "source": [
        "\n",
        "## Create the base model from the pre-trained convnets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19IQ2gqneqmS"
      },
      "source": [
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqcsxoJIEVXZ"
      },
      "source": [
        "\n",
        "Mô hình trích xuất đặc trưng này nhận tấm ảnh với kích thước `160x160x3` chuyển thành đặc trưng `5x5x1280`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-2LJL0EEUcx"
      },
      "source": [
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "feature_batch = base_model(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlx56nQtfe8Y"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnMLieHBCwil"
      },
      "source": [
        "### Freeze the convolutional base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTCJH4bphOeo"
      },
      "source": [
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpbzSmPkDa-N"
      },
      "source": [
        "# Let's take a look at the base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdMRM8YModbk"
      },
      "source": [
        "### Add a classification head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLnpMF5KOALm"
      },
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv4afXKj6cVa"
      },
      "source": [
        "prediction_layer = tf.keras.layers.Dense(NUM_CLASS)\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgzQX6Veb2WT"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ylJXE_kRLi"
      },
      "source": [
        "### Compile the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpR8HdyMhukJ"
      },
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ARiyMFsgbH"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krvBumovycVA"
      },
      "source": [
        "len(model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxvgOYTDSWTx"
      },
      "source": [
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om4O3EESkab1"
      },
      "source": [
        "initial_epochs = 10\n",
        "\n",
        "loss0, accuracy0 = model.evaluate(validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cYT1c48CuSd"
      },
      "source": [
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsaRFlZ9B6WK"
      },
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd94CKImf8vi"
      },
      "source": [
        "### Learning curves\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53OTCh3jnbwV"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqwV-CRdS6Nv"
      },
      "source": [
        "## Fine tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPXnzUK0QonF"
      },
      "source": [
        "### Un-freeze the top layers of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nzcagVitLQm"
      },
      "source": [
        "base_model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4HgVAacRs5v"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at = 100\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uk1dgsxT0IS"
      },
      "source": [
        "### Compile the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtUnaz0WUDva"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwBWy7J2kZvA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNXelbMQtonr"
      },
      "source": [
        "len(model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G5O4jd6TuAG"
      },
      "source": [
        "### Continue training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECQLkAsFTlun"
      },
      "source": [
        "fine_tune_epochs = 10\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(train_dataset,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=history.epoch[-1],\n",
        "                         validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpA8PlpQKygw"
      },
      "source": [
        "acc += history_fine.history['accuracy']\n",
        "val_acc += history_fine.history['val_accuracy']\n",
        "\n",
        "loss += history_fine.history['loss']\n",
        "val_loss += history_fine.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chW103JUItdk"
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6cWgjgfrsn5"
      },
      "source": [
        "### Evaluation and prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KyNhagHwfar"
      },
      "source": [
        "loss, accuracy = model.evaluate(validation_dataset)\n",
        "print('Validation accuracy :', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUNoQNgtfNgt"
      },
      "source": [
        "#Retrieve a batch of images from the test set\n",
        "image_batch, label_batch = validation_dataset.as_numpy_iterator().next()\n",
        "predictions = model.predict_on_batch(image_batch)\n",
        "\n",
        "predicted_class = np.argmax(predictions, axis=-1)\n",
        "\n",
        "\n",
        "print('Predictions:\\n', predicted_class)\n",
        "print('Labels:\\n', label_batch)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
        "  plt.title(class_names[predicted_class[i]])\n",
        "  plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mYFKm3yxLNR"
      },
      "source": [
        "predictions = model.predict(test_dataset)\n",
        "\n",
        "predicted_class = np.argmax(predictions, axis=-1)\n",
        "\n",
        "\n",
        "print('Predictions:\\n', predicted_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh-beiv4sTkE"
      },
      "source": [
        "with open('submission.txt', 'w') as submission_file:\n",
        "  for filename, predicted in zip(test_dataset_filenames, predicted_class):\n",
        "    submission_file.write('{}\\t{}\\n'.format(filename.split('/')[-1], class_names[predicted]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}